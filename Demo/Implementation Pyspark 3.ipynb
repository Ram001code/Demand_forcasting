{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83b5a2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec73092f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d884a425",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "782751ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.101:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1ba99b95eb0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c6816ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    " \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import timedelta\n",
    "    \n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0ddc5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_flagged  =spark.read.option('header','true').csv('Dataset//inventory_flagged_data.csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140b727e",
   "metadata": {},
   "source": [
    "# Step 2: Generate Forecast\n",
    "### Define Function to Generate Forecast for a Store-SKU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46399d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_value = 0.8 # smoothing factor\n",
    " \n",
    "# function to generate a forecast for a store-sku\n",
    "def get_forecast(keys, inventory_pd: pd.DataFrame) -> pd.DataFrame:\n",
    "  \n",
    "    # identify store and sku\n",
    "    store_id = keys[0]\n",
    "    sku = keys[1]\n",
    "\n",
    "    # identify date range for predictions\n",
    "    history_start = inventory_pd['date'].min()\n",
    "    history_end = inventory_pd['date'].max()\n",
    "\n",
    "    # organize data for model training\n",
    "    timeseries = (\n",
    "    inventory_pd\n",
    "      .set_index('date', drop=True, append=False) # move date to index\n",
    "      .sort_index() # sort on date-index\n",
    "    )['total_sales_units'] # just need this one field\n",
    "\n",
    "    # fit model to timeseries\n",
    "    model = SimpleExpSmoothing(timeseries, initialization_method='heuristic').fit(smoothing_level=alpha_value)\n",
    "\n",
    "    # predict sales across historical period\n",
    "    predictions = model.predict(start=history_start, end=history_end)\n",
    "\n",
    "    # convert timeseries to dataframe for return\n",
    "    predictions_pd = predictions.to_frame(name='predicted_sales_units').reset_index() # convert to df\n",
    "    predictions_pd.rename(columns={'index':'date'}, inplace=True) # rename 'index' column to 'date'\n",
    "    predictions_pd['store_id'] = store_id # assign store id\n",
    "    predictions_pd['sku'] = sku # assign sku\n",
    "\n",
    "    return predictions_pd[['date', 'store_id', 'sku', 'predicted_sales_units']]\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b950002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of forecast function output\n",
    "forecast_schema = StructType([\n",
    "  StructField('date', DateType()), \n",
    "  StructField('store_id', IntegerType()), \n",
    "  StructField('sku', IntegerType()), \n",
    "  StructField('predicted_sales_units', FloatType())\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7602ce3a",
   "metadata": {},
   "source": [
    "### Generate Forecasts for All Store-SKUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5c6ccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get forecasted values for each store-sku combination\n",
    "\n",
    "forecast = (\n",
    "  inventory_flagged\n",
    "    .groupby(['store_id','sku'])\n",
    "      .applyInPandas(\n",
    "        get_forecast, \n",
    "        schema=forecast_schema\n",
    "        )\n",
    "    .withColumn('predicted_sales_units', f.expr('ROUND(predicted_sales_units,0)')) # round values to nearest integer\n",
    "    )\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97fb1311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---------------------+\n",
      "|      date|store_id|sku|predicted_sales_units|\n",
      "+----------+--------+---+---------------------+\n",
      "|2019-01-01|      63| 57|                  0.0|\n",
      "|2019-01-02|      63| 57|                  0.0|\n",
      "|2019-01-03|      63| 57|                  0.0|\n",
      "|2019-01-04|      63| 57|                  0.0|\n",
      "|2019-01-05|      63| 57|                  0.0|\n",
      "|2019-01-06|      63| 57|                  2.0|\n",
      "|2019-01-07|      63| 57|                  2.0|\n",
      "|2019-01-08|      63| 57|                  0.0|\n",
      "|2019-01-09|      63| 57|                  0.0|\n",
      "|2019-01-10|      63| 57|                  2.0|\n",
      "|2019-01-11|      63| 57|                  2.0|\n",
      "|2019-01-12|      63| 57|                  0.0|\n",
      "|2019-01-13|      63| 57|                  0.0|\n",
      "|2019-01-14|      63| 57|                  0.0|\n",
      "|2019-01-15|      63| 57|                  0.0|\n",
      "|2019-01-16|      63| 57|                  0.0|\n",
      "|2019-01-17|      63| 57|                  0.0|\n",
      "|2019-01-18|      63| 57|                  0.0|\n",
      "|2019-01-19|      63| 57|                  0.0|\n",
      "|2019-01-20|      63| 57|                  0.0|\n",
      "+----------+--------+---+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "forecast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bcf0b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast.toPandas().to_csv(\"Dataset//inventory_forecast.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c4cbb",
   "metadata": {},
   "source": [
    "# Step 3: Identify Off Sales Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58722215",
   "metadata": {},
   "source": [
    "### Flag Off-Sales Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "96c4acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inventory_forecast = spark.read.option('header','true').csv('Dataset//inventory_forecast.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7245ad51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+---+---------------------+\n",
      "|               date|store_id|sku|predicted_sales_units|\n",
      "+-------------------+--------+---+---------------------+\n",
      "|2019-01-01 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-02 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-03 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-04 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-05 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-06 00:00:00|      63| 57|                  2.0|\n",
      "|2019-01-07 00:00:00|      63| 57|                  2.0|\n",
      "|2019-01-08 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-09 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-10 00:00:00|      63| 57|                  2.0|\n",
      "|2019-01-11 00:00:00|      63| 57|                  2.0|\n",
      "|2019-01-12 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-13 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-14 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-15 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-16 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-17 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-18 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-19 00:00:00|      63| 57|                  0.0|\n",
      "|2019-01-20 00:00:00|      63| 57|                  0.0|\n",
      "+-------------------+--------+---+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_forecast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1b363fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "inventory_forecast = inventory_forecast.withColumn('date', to_date(col('Date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0f0eb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---------------------+\n",
      "|      date|store_id|sku|predicted_sales_units|\n",
      "+----------+--------+---+---------------------+\n",
      "|2019-01-01|      63| 57|                  0.0|\n",
      "|2019-01-02|      63| 57|                  0.0|\n",
      "|2019-01-03|      63| 57|                  0.0|\n",
      "|2019-01-04|      63| 57|                  0.0|\n",
      "|2019-01-05|      63| 57|                  0.0|\n",
      "|2019-01-06|      63| 57|                  2.0|\n",
      "|2019-01-07|      63| 57|                  2.0|\n",
      "|2019-01-08|      63| 57|                  0.0|\n",
      "|2019-01-09|      63| 57|                  0.0|\n",
      "|2019-01-10|      63| 57|                  2.0|\n",
      "|2019-01-11|      63| 57|                  2.0|\n",
      "|2019-01-12|      63| 57|                  0.0|\n",
      "|2019-01-13|      63| 57|                  0.0|\n",
      "|2019-01-14|      63| 57|                  0.0|\n",
      "|2019-01-15|      63| 57|                  0.0|\n",
      "|2019-01-16|      63| 57|                  0.0|\n",
      "|2019-01-17|      63| 57|                  0.0|\n",
      "|2019-01-18|      63| 57|                  0.0|\n",
      "|2019-01-19|      63| 57|                  0.0|\n",
      "|2019-01-20|      63| 57|                  0.0|\n",
      "+----------+--------+---+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inventory_forecast.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c585b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "osa_flag_output = (\n",
    "  \n",
    "  inventory_flagged.alias('inv')\n",
    "    .join(inventory_forecast.alias('for'), on=['store_id','sku','date'], how='leftouter')\n",
    "    .selectExpr(\n",
    "      'inv.*',\n",
    "      'for.predicted_sales_units'\n",
    "      )\n",
    "             \n",
    "    # calculating difference between forecasted and actual sales units\n",
    "    .withColumn('units_difference', f.expr('predicted_sales_units - total_sales_units'))\n",
    "    .withColumn('units_difference', f.expr('COALESCE(units_difference, 0)'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac21dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "osa_flag_output = (\n",
    "  \n",
    "    osa_flag_output\n",
    "    # check whether deviation has been increasing over past 4 days\n",
    "    .withColumn('osa_alert_inc_deviation', f.expr('''\n",
    "      CASE \n",
    "        WHEN units_difference > LAG(units_difference, 1) OVER(PARTITION BY store_id, sku ORDER BY date) AND \n",
    "             LAG(units_difference, 1) OVER(PARTITION BY store_id, sku ORDER BY date) > LAG(units_difference, 2) OVER(PARTITION BY store_id, sku ORDER BY date) AND \n",
    "             LAG(units_difference, 2) OVER(PARTITION BY store_id, sku ORDER BY date) > LAG(units_difference, 3) OVER(PARTITION BY store_id, sku ORDER BY date)\n",
    "             THEN 1\n",
    "        ELSE 0 \n",
    "        END'''))\n",
    "    .withColumn('osa_alert_inc_deviation', f.expr('COALESCE(osa_alert_inc_deviation, 0)'))\n",
    " \n",
    "    # rolling 4 day average of sales units\n",
    "    .withColumn('sales_4day_avg', f.expr('AVG(total_sales_units) OVER(PARTITION BY store_id, sku ORDER BY date ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)'))\n",
    " \n",
    "    # rolling 4 day average of forecasted units\n",
    "    .withColumn('predictions_4day_avg', f.expr('AVG(predicted_sales_units) OVER(PARTITION BY store_id, sku ORDER BY date ROWS BETWEEN 3 PRECEDING AND CURRENT ROW)'))\n",
    " \n",
    "    # calculating deviation in rolling average of sales and forecast units\n",
    "    .withColumn('deviation', f.expr('(predictions_4day_avg - sales_4day_avg) / (predictions_4day_avg+1)'))\n",
    "    .withColumn('deviation', f.expr('COALESCE(deviation, 0)'))\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dcff9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "osa_flag_output = (\n",
    "  \n",
    "    osa_flag_output\n",
    "# Considering 20% deviation as the threshold for OSA flag\n",
    "    .withColumn('off_sales_alert', f.expr('''\n",
    "      CASE \n",
    "        WHEN deviation > 0.20  AND osa_alert_inc_deviation = 1 THEN 1\n",
    "        ELSE 0\n",
    "        END'''))\n",
    " \n",
    "    .select('date', \n",
    "            'store_id', \n",
    "            'sku', \n",
    "            'predicted_sales_units', \n",
    "            'off_sales_alert',\n",
    "            'oos_alert', \n",
    "            'zero_sales_flag', \n",
    "            'phantom_inventory', \n",
    "            'phantom_inventory_ind')\n",
    "    )\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "13396651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "|               date|store_id|sku|predicted_sales_units|off_sales_alert|oos_alert|zero_sales_flag|phantom_inventory|phantom_inventory_ind|\n",
      "+-------------------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "|2019-01-01 00:00:00|      63| 57|                  0.0|              0|        0|              0|             null|                    0|\n",
      "|2019-01-02 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-03 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-04 00:00:00|      63| 57|                  0.0|              0|        0|              0|              1.0|                    1|\n",
      "|2019-01-05 00:00:00|      63| 57|                  0.0|              0|        0|              0|              7.0|                    1|\n",
      "|2019-01-06 00:00:00|      63| 57|                  2.0|              0|        0|              0|             -8.0|                    1|\n",
      "|2019-01-07 00:00:00|      63| 57|                  2.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-08 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-09 00:00:00|      63| 57|                  0.0|              0|        0|              0|              1.0|                    0|\n",
      "|2019-01-10 00:00:00|      63| 57|                  2.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-11 00:00:00|      63| 57|                  2.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-12 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-13 00:00:00|      63| 57|                  0.0|              0|        0|              0|             -8.0|                    1|\n",
      "|2019-01-14 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-15 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-16 00:00:00|      63| 57|                  0.0|              0|        0|              0|             -1.0|                    0|\n",
      "|2019-01-17 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-18 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-19 00:00:00|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-20 00:00:00|      63| 57|                  0.0|              0|        0|              0|              6.0|                    1|\n",
      "+-------------------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osa_flag_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07e58822",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "osa_flag_output = osa_flag_output.withColumn('date', to_date(col('Date')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2e8e65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "|      date|store_id|sku|predicted_sales_units|off_sales_alert|oos_alert|zero_sales_flag|phantom_inventory|phantom_inventory_ind|\n",
      "+----------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "|2019-01-01|      63| 57|                  0.0|              0|        0|              0|             null|                    0|\n",
      "|2019-01-02|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-03|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-04|      63| 57|                  0.0|              0|        0|              0|              1.0|                    1|\n",
      "|2019-01-05|      63| 57|                  0.0|              0|        0|              0|              7.0|                    1|\n",
      "|2019-01-06|      63| 57|                  2.0|              0|        0|              0|             -8.0|                    1|\n",
      "|2019-01-07|      63| 57|                  2.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-08|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-09|      63| 57|                  0.0|              0|        0|              0|              1.0|                    0|\n",
      "|2019-01-10|      63| 57|                  2.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-11|      63| 57|                  2.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-12|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-13|      63| 57|                  0.0|              0|        0|              0|             -8.0|                    1|\n",
      "|2019-01-14|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-15|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-16|      63| 57|                  0.0|              0|        0|              0|             -1.0|                    0|\n",
      "|2019-01-17|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-18|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-19|      63| 57|                  0.0|              0|        0|              0|              0.0|                    0|\n",
      "|2019-01-20|      63| 57|                  0.0|              0|        0|              0|              6.0|                    1|\n",
      "+----------+--------+---+---------------------+---------------+---------+---------------+-----------------+---------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "osa_flag_output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7f97d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "osa_flag_output.toPandas().to_csv(\"Dataset//osa_flag_outputt.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c351f64f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
